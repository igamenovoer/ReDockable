{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to PeiDocker Don't keep your docker images around, keep the build files! If you ever want to make reproducible docker images but have no patience to learn Dockerfiles and docker-compose, PeiDocker is for you. PeiDocker (\u914d docker) helps you script and organize your docker image building process without learning too much about Dockerfiles and docker-compose, it streamlines the building process and allows you to customize the image building and running behaviours using shell scripts. With PeiDocker, you can: Build images with SSH support. Install packages from public repository using proxy. Easily choose to install apps into the image, your host directory, or docker volumes, you can also switch between them after the image is built. Run custom commands during image building, such as setting up environment variables, installing packages, etc. Run custom commands when the container starts. How to use Install dependencies: pip install click omegaconf attrs cattrs Create a new project: # cd to the root of the git repository cd /path/to/PeiDocker # Create a new project in ./build or any other directory python -m pei_docker.pei create -p ./build Edit the configuration file user_config.yml in the project directory (e.g., ./build ) according to your needs. Generate the docker-compose.yml file in the project directory: python -m pei_docker.pei configure -p ./build Build the docker images. There are two images to be built, namely stage-1 and stage-2 . stage-1 is intended to be a base image, installing system apps using apt install , stage-2 is intended to be a final image based on stage-1 , installing custom apps using downloaded packages like .deb . External storage is only available in stage-2 . cd ./build # Using docker compose to build the images. # To see all the output, use --progress=plain # To cleanly rebuild the images, use --no-cache # Build the stage-1 image # By default, the image is named pei-image:stage-1, you can change it in user_config.yml docker-compose build stage-1 --progress=plain # Build the stage-2 image # By default, the image is named pei-image:stage-2 docker-compose build stage-2 --progress=plain Run the docker container: # inside project directory, such as ./build # Typically you will run the stage-2 container # You can also up the stage-1 container as well. docker compose up stage-2 If you have setup SSH in user_config.yml , now you can SSH into the container: # by default, it will create a user named `me` with password '123456' # and map the port 2222 to the container's port 22 ssh me@127.0.0.1 -p 2222 That's it, you are good to go. If you prefer to run the image using docker run instead of docker compose , you can convert the docker-compose.yml to commands using Decomposerize . If you have trouble connecting to docker.io when building the image, you can either set the global proxy for docker, or pull the base image manually and tag it with a local name. For detail, see stack overflow . # get the base image manually docker pull ubuntu:24.04 # tag it with a local name, # in user_config.yml, use my-ubuntu:24.04 as the base image # to skip checking with docker.io docker tag ubuntu:24.04 my-ubuntu:24.04 Custom commands To run custom commands during build, edit the scripts in <project_dir>/installation/stage-<1,2>/custom . You can also run other scripts by adding them in user_config.yml . If you have on-first-run commands in user_config.yml , after the first run, you shall commit the container to a new image so that the changes are saved, or otherwise those commands will be executed again when the container is recreated. Stage-2 storage The image built in stage-2 have the following directories for user: /soft/app : the directory to store the installed apps. /soft/data : the directory to store the data files. /soft/workspace : the directory to store the workspace files, like code. These /soft/xxx are links to the corresponding directories in /hard/image/xxx (in-image storage) or /hard/volume/xxx (external storage), where content is actually stored, based on the following rules: If /hard/volume/xxx is found, then /soft/xxx is a link to /hard/volume/xxx . Otherwise, if /hard/image/xxx is found, then /soft/xxx is a link to /hard/image/xxx . As such, you can switch between in-image storage and external storage by mounting into /hard/volume/xxx , and this behavious is already present in the generated docker-compose.yml file based on your user_config.yml . Note that only predefined directories ( app , data , workspace ) are linked, others will be ignored. Project directory layout Inside your project directory, you will find the following files and directories: compose-template.yml # the template for docker-compose.yml, do not modify this file stage-1.Dockerfile # the Dockerfile for stage-1 image, do not modify this file stage-2.Dockerfile # the Dockerfile for stage-2 image, do not modify this file user_config.yml # the configuration file, edit this file to customize installation/ # this directory will be copied into the image under /pei-init stage-1/ # the scripts to run during stage-1 image building custom/ # the custom scripts to run during stage-1 image building, you need to explicitly list them in user_config.yml tmp/ # the temporary directory to store downloaded packages, like .deb, .whl, etc. system/ # the configuration files for system apps, like apt, pip, etc. ... stage-2/ # the scripts to run during stage-2 image building custom/ # the custom scripts to run during stage-2 image building, you need to explicitly list them in user_config.yml tmp/ # the temporary directory to store downloaded packages, like .deb, .whl, etc. system/ # the configuration files for system apps, like apt, pip, etc. ... User configuration file Here are the options you can set in user_config.yml : # all paths are relative to /installation directory stage_1: # input/output image settings image: base: ubuntu:24.04 output: pei-image:stage-1 # ssh settings ssh: enable: true port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' # public key file path, relative to the installation directory # e.g., 'stage-1/system/ssh/keys/mykey.rsa.pub' pubkey_file: null you: password: '654321' pubkey_file: null root: # you can configure root user here password: root pubkey_file: null # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: host.docker.internal # default value, this will map to the host machine port: 7890 # if address==host.docker.internal, this will be the proxy port on host machine enable_globally: false # enable proxy for all shell commands during build and run? remove_after_build: false # remove global proxy after build? use_https: false # use https proxy? # apt settings apt: # replace the default apt source with a custom one, use empty string to disable this # repo_source: 'stage-1/system/apt/ubuntu-22.04-tsinghua-x64.list' # special values that refer to well known apt sources: # 'tuna' : 'http://mirrors.tuna.tsinghua.edu.cn/ubuntu/' # 'aliyun' : 'http://mirrors.aliyun.com/ubuntu/' # '163' : 'http://mirrors.163.com/ubuntu/' # 'ustc' : 'http://mirrors.ustc.edu.cn/ubuntu/' # 'cn' : 'http://cn.archive.ubuntu.com/ubuntu/ repo_source: '' keep_repo_after_build: true # keep the apt source file after build? use_proxy: false # use proxy for apt? keep_proxy_after_build: false # keep proxy settings after build? # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: - 'EXAMPLE_VAR_STAGE_1=example env var' # additional port mapping # see https://docs.docker.com/compose/networking/ ports: [] # device settings device: type: cpu # can be cpu or gpu # mount external volumes to container # the volumes can be given any names, mounted anywhere # mount section does NOT transfer to the next stage, so you need to define them again in stage-2 mount: apt_cache: type: auto-volume dst_path: /var/cache/apt host_path: null volume_name: null # custom scripts custom: # scripts run during build on_build: - 'stage-1/custom/install-dev-tools.sh' # just an example, you can safely remove this - 'stage-1/custom/my-build-1.sh' - 'stage-1/custom/my-build-2.sh' # scripts run on first run on_first_run: - 'stage-1/custom/my-on-first-run-1.sh' - 'stage-1/custom/my-on-first-run-2.sh' # scripts run on every run on_every_run: - 'stage-1/custom/my-on-every-run-1.sh' - 'stage-1/custom/my-on-every-run-2.sh' # scripts run on user login on_user_login: - 'stage-1/custom/my-on-user-login-1.sh' - 'stage-1/custom/my-on-user-login-2.sh' stage_2: # input/output image settings image: base: null # if not specified, use the output image of stage-1 output: pei-image:stage-2 # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: # use list intead of dict - 'EXAMPLE_VAR_STAGE_2=example env var' # port mapping, will be appended to the stage-1 port mapping # see https://docs.docker.com/compose/networking/ ports: [] # device settings, will override the stage-1 device settings device: type: cpu # can be cpu or gpu # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: null # this means to use the proxy settings of stage-1 port: null enable_globally: null remove_after_build: null use_https: null # storage configurations storage: app: type: auto-volume # auto-volume, manual-volume, host, image host_path: null # host directory to be mounted, in effect when type=host volume_name: null # volume name, in effect when type=manual-volume data: type: auto-volume host_path: null volume_name: null workspace: type: auto-volume host_path: null volume_name: null # mount external volumes to container # the volumes can be given any names, mounted anywhere # the volume type cannot be 'image', or otherwise it will be ignored mount: home_me: type: auto-volume # auto-volume, manual-volume, host dst_path: /home/me host_path: null volume_name: null apt_cache: type: auto-volume dst_path: /var/cache/apt host_path: null volume_name: null # custom scripts in stage-2, run after stage-1 custom scripts custom: # scripts run during build on_build: - 'stage-2/custom/install-gui-tools.sh' # just an example, you can safely remove this - 'stage-2/custom/my-build-1.sh' - 'stage-2/custom/my-build-2.sh' # scripts run on first start on_first_run: - 'stage-2/custom/my-on-first-run-1.sh' - 'stage-2/custom/my-on-first-run-2.sh' # scripts run on every start on_every_run: - 'stage-2/custom/my-on-every-run-1.sh' - 'stage-2/custom/my-on-every-run-2.sh' # scripts run on user login on_user_login: - 'stage-2/custom/my-on-user-login-1.sh' - 'stage-2/custom/my-on-user-login-2.sh'","title":"Introduction"},{"location":"#welcome-to-peidocker","text":"Don't keep your docker images around, keep the build files! If you ever want to make reproducible docker images but have no patience to learn Dockerfiles and docker-compose, PeiDocker is for you. PeiDocker (\u914d docker) helps you script and organize your docker image building process without learning too much about Dockerfiles and docker-compose, it streamlines the building process and allows you to customize the image building and running behaviours using shell scripts. With PeiDocker, you can: Build images with SSH support. Install packages from public repository using proxy. Easily choose to install apps into the image, your host directory, or docker volumes, you can also switch between them after the image is built. Run custom commands during image building, such as setting up environment variables, installing packages, etc. Run custom commands when the container starts.","title":"Welcome to PeiDocker"},{"location":"#how-to-use","text":"Install dependencies: pip install click omegaconf attrs cattrs Create a new project: # cd to the root of the git repository cd /path/to/PeiDocker # Create a new project in ./build or any other directory python -m pei_docker.pei create -p ./build Edit the configuration file user_config.yml in the project directory (e.g., ./build ) according to your needs. Generate the docker-compose.yml file in the project directory: python -m pei_docker.pei configure -p ./build Build the docker images. There are two images to be built, namely stage-1 and stage-2 . stage-1 is intended to be a base image, installing system apps using apt install , stage-2 is intended to be a final image based on stage-1 , installing custom apps using downloaded packages like .deb . External storage is only available in stage-2 . cd ./build # Using docker compose to build the images. # To see all the output, use --progress=plain # To cleanly rebuild the images, use --no-cache # Build the stage-1 image # By default, the image is named pei-image:stage-1, you can change it in user_config.yml docker-compose build stage-1 --progress=plain # Build the stage-2 image # By default, the image is named pei-image:stage-2 docker-compose build stage-2 --progress=plain Run the docker container: # inside project directory, such as ./build # Typically you will run the stage-2 container # You can also up the stage-1 container as well. docker compose up stage-2 If you have setup SSH in user_config.yml , now you can SSH into the container: # by default, it will create a user named `me` with password '123456' # and map the port 2222 to the container's port 22 ssh me@127.0.0.1 -p 2222 That's it, you are good to go. If you prefer to run the image using docker run instead of docker compose , you can convert the docker-compose.yml to commands using Decomposerize . If you have trouble connecting to docker.io when building the image, you can either set the global proxy for docker, or pull the base image manually and tag it with a local name. For detail, see stack overflow . # get the base image manually docker pull ubuntu:24.04 # tag it with a local name, # in user_config.yml, use my-ubuntu:24.04 as the base image # to skip checking with docker.io docker tag ubuntu:24.04 my-ubuntu:24.04","title":"How to use"},{"location":"#custom-commands","text":"To run custom commands during build, edit the scripts in <project_dir>/installation/stage-<1,2>/custom . You can also run other scripts by adding them in user_config.yml . If you have on-first-run commands in user_config.yml , after the first run, you shall commit the container to a new image so that the changes are saved, or otherwise those commands will be executed again when the container is recreated.","title":"Custom commands"},{"location":"#stage-2-storage","text":"The image built in stage-2 have the following directories for user: /soft/app : the directory to store the installed apps. /soft/data : the directory to store the data files. /soft/workspace : the directory to store the workspace files, like code. These /soft/xxx are links to the corresponding directories in /hard/image/xxx (in-image storage) or /hard/volume/xxx (external storage), where content is actually stored, based on the following rules: If /hard/volume/xxx is found, then /soft/xxx is a link to /hard/volume/xxx . Otherwise, if /hard/image/xxx is found, then /soft/xxx is a link to /hard/image/xxx . As such, you can switch between in-image storage and external storage by mounting into /hard/volume/xxx , and this behavious is already present in the generated docker-compose.yml file based on your user_config.yml . Note that only predefined directories ( app , data , workspace ) are linked, others will be ignored.","title":"Stage-2 storage"},{"location":"#project-directory-layout","text":"Inside your project directory, you will find the following files and directories: compose-template.yml # the template for docker-compose.yml, do not modify this file stage-1.Dockerfile # the Dockerfile for stage-1 image, do not modify this file stage-2.Dockerfile # the Dockerfile for stage-2 image, do not modify this file user_config.yml # the configuration file, edit this file to customize installation/ # this directory will be copied into the image under /pei-init stage-1/ # the scripts to run during stage-1 image building custom/ # the custom scripts to run during stage-1 image building, you need to explicitly list them in user_config.yml tmp/ # the temporary directory to store downloaded packages, like .deb, .whl, etc. system/ # the configuration files for system apps, like apt, pip, etc. ... stage-2/ # the scripts to run during stage-2 image building custom/ # the custom scripts to run during stage-2 image building, you need to explicitly list them in user_config.yml tmp/ # the temporary directory to store downloaded packages, like .deb, .whl, etc. system/ # the configuration files for system apps, like apt, pip, etc. ...","title":"Project directory layout"},{"location":"#user-configuration-file","text":"Here are the options you can set in user_config.yml : # all paths are relative to /installation directory stage_1: # input/output image settings image: base: ubuntu:24.04 output: pei-image:stage-1 # ssh settings ssh: enable: true port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' # public key file path, relative to the installation directory # e.g., 'stage-1/system/ssh/keys/mykey.rsa.pub' pubkey_file: null you: password: '654321' pubkey_file: null root: # you can configure root user here password: root pubkey_file: null # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: host.docker.internal # default value, this will map to the host machine port: 7890 # if address==host.docker.internal, this will be the proxy port on host machine enable_globally: false # enable proxy for all shell commands during build and run? remove_after_build: false # remove global proxy after build? use_https: false # use https proxy? # apt settings apt: # replace the default apt source with a custom one, use empty string to disable this # repo_source: 'stage-1/system/apt/ubuntu-22.04-tsinghua-x64.list' # special values that refer to well known apt sources: # 'tuna' : 'http://mirrors.tuna.tsinghua.edu.cn/ubuntu/' # 'aliyun' : 'http://mirrors.aliyun.com/ubuntu/' # '163' : 'http://mirrors.163.com/ubuntu/' # 'ustc' : 'http://mirrors.ustc.edu.cn/ubuntu/' # 'cn' : 'http://cn.archive.ubuntu.com/ubuntu/ repo_source: '' keep_repo_after_build: true # keep the apt source file after build? use_proxy: false # use proxy for apt? keep_proxy_after_build: false # keep proxy settings after build? # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: - 'EXAMPLE_VAR_STAGE_1=example env var' # additional port mapping # see https://docs.docker.com/compose/networking/ ports: [] # device settings device: type: cpu # can be cpu or gpu # mount external volumes to container # the volumes can be given any names, mounted anywhere # mount section does NOT transfer to the next stage, so you need to define them again in stage-2 mount: apt_cache: type: auto-volume dst_path: /var/cache/apt host_path: null volume_name: null # custom scripts custom: # scripts run during build on_build: - 'stage-1/custom/install-dev-tools.sh' # just an example, you can safely remove this - 'stage-1/custom/my-build-1.sh' - 'stage-1/custom/my-build-2.sh' # scripts run on first run on_first_run: - 'stage-1/custom/my-on-first-run-1.sh' - 'stage-1/custom/my-on-first-run-2.sh' # scripts run on every run on_every_run: - 'stage-1/custom/my-on-every-run-1.sh' - 'stage-1/custom/my-on-every-run-2.sh' # scripts run on user login on_user_login: - 'stage-1/custom/my-on-user-login-1.sh' - 'stage-1/custom/my-on-user-login-2.sh' stage_2: # input/output image settings image: base: null # if not specified, use the output image of stage-1 output: pei-image:stage-2 # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: # use list intead of dict - 'EXAMPLE_VAR_STAGE_2=example env var' # port mapping, will be appended to the stage-1 port mapping # see https://docs.docker.com/compose/networking/ ports: [] # device settings, will override the stage-1 device settings device: type: cpu # can be cpu or gpu # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: null # this means to use the proxy settings of stage-1 port: null enable_globally: null remove_after_build: null use_https: null # storage configurations storage: app: type: auto-volume # auto-volume, manual-volume, host, image host_path: null # host directory to be mounted, in effect when type=host volume_name: null # volume name, in effect when type=manual-volume data: type: auto-volume host_path: null volume_name: null workspace: type: auto-volume host_path: null volume_name: null # mount external volumes to container # the volumes can be given any names, mounted anywhere # the volume type cannot be 'image', or otherwise it will be ignored mount: home_me: type: auto-volume # auto-volume, manual-volume, host dst_path: /home/me host_path: null volume_name: null apt_cache: type: auto-volume dst_path: /var/cache/apt host_path: null volume_name: null # custom scripts in stage-2, run after stage-1 custom scripts custom: # scripts run during build on_build: - 'stage-2/custom/install-gui-tools.sh' # just an example, you can safely remove this - 'stage-2/custom/my-build-1.sh' - 'stage-2/custom/my-build-2.sh' # scripts run on first start on_first_run: - 'stage-2/custom/my-on-first-run-1.sh' - 'stage-2/custom/my-on-first-run-2.sh' # scripts run on every start on_every_run: - 'stage-2/custom/my-on-every-run-1.sh' - 'stage-2/custom/my-on-every-run-2.sh' # scripts run on user login on_user_login: - 'stage-2/custom/my-on-user-login-1.sh' - 'stage-2/custom/my-on-user-login-2.sh'","title":"User configuration file"},{"location":"advanced_examples/","text":"Advanced Examples Hardware-accelerated OpenGL Windows The following example demonstrates how to build a Docker image with hardware-accelerated OpenGL support on Windows. The critical part is to: install necessary packages (e.g., libglvnd-dev ) in the Dockerfile, see setup-opengl-win32.sh mount the necessary directories for WSLg to work properly, see WSLg GPU selection . # all paths are relative to /installation directory # for windows, you need to add these to the final docker-compose.yml file # see https://github.com/microsoft/wslg/wiki/GPU-selection-in-WSLg # volumes: # - /tmp/.X11-unix:/tmp/.X11-unix # - /mnt/wslg:/mnt/wslg # - /usr/lib/wsl:/usr/lib/wsl # - /dev:/dev stage_1: # input/output image settings image: base: nvidia/cuda:12.3.2-base-ubuntu22.04 output: pei-opengl:stage-1 # ssh settings ssh: enable: true port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: host.docker.internal # default value, this will map to the host machine port: 7890 # if address==host.docker.internal, this will be the proxy port on host machine enable_globally: false # enable proxy for all shell commands during build and run? remove_after_build: false # remove global proxy after build? use_https: false # use https proxy? # apt settings apt: repo_source: 'tuna' # device settings device: type: gpu # can be cpu or gpu stage_2: # input/output image settings image: output: pei-opengl:stage-2 # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: # use list intead of dict NVIDIA_VISIBLE_DEVICES: all NVIDIA_DRIVER_CAPABILITIES: all # device settings, will override the stage-1 device settings device: type: gpu # can be cpu or gpu storage: app: type: auto-volume data: type: auto-volume workspace: type: auto-volume # custom scripts in stage-2, run after stage-1 custom scripts custom: # scripts run during build on_build: - 'stage-2/system/opengl/setup-opengl-win32.sh' # install opengl # this is the docker-compose.yml file version: '3' services: stage-2: image: pei-opengl:stage-2 stdin_open: true tty: true command: /bin/bash deploy: resources: reservations: devices: - driver: nvidia capabilities: - gpu extra_hosts: - host.docker.internal:host-gateway build: context: . dockerfile: stage-2.Dockerfile extra_hosts: - host.docker.internal:host-gateway args: BASE_IMAGE: pei-opengl:stage-1 WITH_ESSENTIAL_APPS: true INSTALL_DIR_HOST_2: ./installation/stage-2 INSTALL_DIR_CONTAINER_2: /pei-init/stage-2 PEI_PREFIX_APPS: app PEI_PREFIX_DATA: data PEI_PREFIX_WORKSPACE: workspace PEI_PREFIX_VOLUME: volume PEI_PREFIX_IMAGE: image PEI_PATH_HARD: /hard PEI_PATH_SOFT: /soft PEI_HTTP_PROXY_2: http://host.docker.internal:7890 PEI_HTTPS_PROXY_2: http://host.docker.internal:7890 ENABLE_GLOBAL_PROXY: false REMOVE_GLOBAL_PROXY_AFTER_BUILD: false ports: - '2222:22' environment: NVIDIA_VISIBLE_DEVICES: all NVIDIA_DRIVER_CAPABILITIES: all volumes: - app:/hard/volume/app - data:/hard/volume/data - workspace:/hard/volume/workspace - /tmp/.X11-unix:/tmp/.X11-unix # for wslg - /mnt/wslg:/mnt/wslg # for wslg - /usr/lib/wsl:/usr/lib/wsl # for wslg - /dev:/dev # for wslg stage-1: image: pei-opengl:stage-1 stdin_open: true tty: true command: /bin/bash deploy: resources: reservations: devices: - driver: nvidia capabilities: - gpu environment: {} extra_hosts: - host.docker.internal:host-gateway build: context: . dockerfile: stage-1.Dockerfile extra_hosts: - host.docker.internal:host-gateway args: BASE_IMAGE: nvidia/cuda:12.3.2-base-ubuntu22.04 WITH_ESSENTIAL_APPS: true WITH_SSH: true SSH_USER_NAME: me SSH_USER_PASSWORD: '123456' SSH_PUBKEY_FILE: '' APT_SOURCE_FILE: tuna KEEP_APT_SOURCE_FILE: true APT_USE_PROXY: false APT_KEEP_PROXY: false PEI_HTTP_PROXY_1: http://host.docker.internal:7890 PEI_HTTPS_PROXY_1: http://host.docker.internal:7890 ENABLE_GLOBAL_PROXY: false REMOVE_GLOBAL_PROXY_AFTER_BUILD: false INSTALL_DIR_HOST_1: ./installation/stage-1 INSTALL_DIR_CONTAINER_1: /pei-init/stage-1 ROOT_PASSWORD: '' ports: - '2222:22' volumes: app: {} data: {} workspace: {}","title":"Advanced Examples"},{"location":"advanced_examples/#advanced-examples","text":"","title":"Advanced Examples"},{"location":"advanced_examples/#hardware-accelerated-opengl","text":"","title":"Hardware-accelerated OpenGL"},{"location":"advanced_examples/#windows","text":"The following example demonstrates how to build a Docker image with hardware-accelerated OpenGL support on Windows. The critical part is to: install necessary packages (e.g., libglvnd-dev ) in the Dockerfile, see setup-opengl-win32.sh mount the necessary directories for WSLg to work properly, see WSLg GPU selection . # all paths are relative to /installation directory # for windows, you need to add these to the final docker-compose.yml file # see https://github.com/microsoft/wslg/wiki/GPU-selection-in-WSLg # volumes: # - /tmp/.X11-unix:/tmp/.X11-unix # - /mnt/wslg:/mnt/wslg # - /usr/lib/wsl:/usr/lib/wsl # - /dev:/dev stage_1: # input/output image settings image: base: nvidia/cuda:12.3.2-base-ubuntu22.04 output: pei-opengl:stage-1 # ssh settings ssh: enable: true port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' # proxy settings # inside the container, the proxy will accessed as http://{address}:{port} # note that whether the proxy is used or not depends on the applications proxy: address: host.docker.internal # default value, this will map to the host machine port: 7890 # if address==host.docker.internal, this will be the proxy port on host machine enable_globally: false # enable proxy for all shell commands during build and run? remove_after_build: false # remove global proxy after build? use_https: false # use https proxy? # apt settings apt: repo_source: 'tuna' # device settings device: type: gpu # can be cpu or gpu stage_2: # input/output image settings image: output: pei-opengl:stage-2 # additional environment variables # see https://docs.docker.com/compose/environment-variables/set-environment-variables/ environment: # use list intead of dict NVIDIA_VISIBLE_DEVICES: all NVIDIA_DRIVER_CAPABILITIES: all # device settings, will override the stage-1 device settings device: type: gpu # can be cpu or gpu storage: app: type: auto-volume data: type: auto-volume workspace: type: auto-volume # custom scripts in stage-2, run after stage-1 custom scripts custom: # scripts run during build on_build: - 'stage-2/system/opengl/setup-opengl-win32.sh' # install opengl # this is the docker-compose.yml file version: '3' services: stage-2: image: pei-opengl:stage-2 stdin_open: true tty: true command: /bin/bash deploy: resources: reservations: devices: - driver: nvidia capabilities: - gpu extra_hosts: - host.docker.internal:host-gateway build: context: . dockerfile: stage-2.Dockerfile extra_hosts: - host.docker.internal:host-gateway args: BASE_IMAGE: pei-opengl:stage-1 WITH_ESSENTIAL_APPS: true INSTALL_DIR_HOST_2: ./installation/stage-2 INSTALL_DIR_CONTAINER_2: /pei-init/stage-2 PEI_PREFIX_APPS: app PEI_PREFIX_DATA: data PEI_PREFIX_WORKSPACE: workspace PEI_PREFIX_VOLUME: volume PEI_PREFIX_IMAGE: image PEI_PATH_HARD: /hard PEI_PATH_SOFT: /soft PEI_HTTP_PROXY_2: http://host.docker.internal:7890 PEI_HTTPS_PROXY_2: http://host.docker.internal:7890 ENABLE_GLOBAL_PROXY: false REMOVE_GLOBAL_PROXY_AFTER_BUILD: false ports: - '2222:22' environment: NVIDIA_VISIBLE_DEVICES: all NVIDIA_DRIVER_CAPABILITIES: all volumes: - app:/hard/volume/app - data:/hard/volume/data - workspace:/hard/volume/workspace - /tmp/.X11-unix:/tmp/.X11-unix # for wslg - /mnt/wslg:/mnt/wslg # for wslg - /usr/lib/wsl:/usr/lib/wsl # for wslg - /dev:/dev # for wslg stage-1: image: pei-opengl:stage-1 stdin_open: true tty: true command: /bin/bash deploy: resources: reservations: devices: - driver: nvidia capabilities: - gpu environment: {} extra_hosts: - host.docker.internal:host-gateway build: context: . dockerfile: stage-1.Dockerfile extra_hosts: - host.docker.internal:host-gateway args: BASE_IMAGE: nvidia/cuda:12.3.2-base-ubuntu22.04 WITH_ESSENTIAL_APPS: true WITH_SSH: true SSH_USER_NAME: me SSH_USER_PASSWORD: '123456' SSH_PUBKEY_FILE: '' APT_SOURCE_FILE: tuna KEEP_APT_SOURCE_FILE: true APT_USE_PROXY: false APT_KEEP_PROXY: false PEI_HTTP_PROXY_1: http://host.docker.internal:7890 PEI_HTTPS_PROXY_1: http://host.docker.internal:7890 ENABLE_GLOBAL_PROXY: false REMOVE_GLOBAL_PROXY_AFTER_BUILD: false INSTALL_DIR_HOST_1: ./installation/stage-1 INSTALL_DIR_CONTAINER_1: /pei-init/stage-1 ROOT_PASSWORD: '' ports: - '2222:22' volumes: app: {} data: {} workspace: {}","title":"Windows"},{"location":"examples/","text":"Examples Basic Ubuntu image with SSH support This is the most basic example of creating an image with SSH support. The image is based on ubuntu:24.04 and has three users: me , you , and root . The passwords for the users are 123456 , 654321 , and root respectively. The SSH server is running on port 22 and mapped to host port 2222 . To accelerate apt installation in China, the apt source is set to tuna . For other options, see the full config file for documentation. If this option is omitted, the default source will be used. # user_config.yml under project directory stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root apt: repo_source: tuna With this user_config.yml in side your project dir, do the followings to build and run the image: # assuming the project dir is /path/to/project python -m pei_docker.pei configure --project-dir=/path/to/project cd /path/to/project docker compose build stage-1 --progress=plain --no-cache docker compose up stage-1 With docker run If you prefer to use docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. The command will look like this: docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 pei-image:stage-1 /bin/bash GPU image with external storage This example is based on the basic ssh image , which demonstrates how to use GPU in the container. The image is based on nvidia/cuda:11.8.0-runtime-ubuntu22.04 that makes use of the GPU. As such, the device section is added to the stage_1 and stage_2 sections, which are set to gpu . stage_1: image: base: nvidia/cuda:11.8.0-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: host host_path: d:/code/PeiDocker/build/storage/app data: type: host host_path: d:/code/PeiDocker/build/storage/data workspace: type: host host_path: d:/code/PeiDocker/build/storage/workspace External storage with host directory The stage-2 image has three external storage directories: app , data , and workspace (note that the directory names are NOT CUSTOMIZABLE , for arbitrary mounts, see the next section ), where specified host directories are mounted. In the container, you can access these directories through /soft/app , /soft/data , and /soft/workspace , which are linked to /app , /data , and /workspace under the /hard/volume . In this example, the host directories are d:/code/PeiDocker/build/storage/app , d:/code/PeiDocker/build/storage/data , and d:/code/PeiDocker/build/storage/workspace (Windows path). Mounting arbitrary volumes or directories You can mount additional volumes or directories to the container by adding them to the mount section. The following example demonstrates how to mount the apt_cache directory to the container's /var/cache/apt directory, in this way, the apt cache will be saved for future use. home_me is mounted to /home/me to save the home directory to a volume, so that it will not get lost after the container is deleted. Unlike app , data , and workspace , the mounted volumes apt_cache and home_me will not be linked to /soft , they are mounted directly to the container and not managed by PeiDocker. You can also use manual-volume (with volume_name set) or host (with host_path set) in ther type though. # heavy duty cpp development, install a lot of things stage_1: # input/output image settings image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 # ssh settings ssh: port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' root: # you can configure root user here password: root pubkey_file: null device: type: gpu apt: repo_source: 'tuna' # additional mounts mount: # save apt cache, to speed up future installations apt_cache: type: auto-volume dst_path: /var/cache/apt stage_2: image: output: pei-image:stage-2 device: type: gpu # additional mounts mount: # save home directory to volume, so that it will not get lost after container deletion home_me: type: auto-volume dst_path: /home/me # mount will not be inherited from stage-1 # you need to mount it again, or otherwise it will not be mounted apt_cache: type: auto-volume dst_path: /var/cache/apt With docker-compose To build and run the image, do the followings: # assuming the project dir is /path/to/project # build stage-1 docker compose -f /path/to/project/docker-compose.yml build stage-1 --progress=plain --no-cache # build stage-2 docker compose -f /path/to/project/docker-compose.yml build stage-2 --progress=plain --no-cache # start stage-2 docker compose -f /path/to/project/docker-compose.yml up stage-2 After that, you can ssh into the container with the following command: ssh -p 2222 me@127.0.0.1 With docker run If you are using docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. The command will look like this, note that you need to add --gpus all manually: # you only need to run stage-2 docker run --gpus all -i -t --add-host host.docker.internal:host-gateway -p 2222:22 -v d:/code/PeiDocker/build/storage/app:/hard/volume/app -v d:/code/PeiDocker/build/storage/data:/hard/volume/data -v d:/code/PeiDocker/build/storage/workspace:/hard/volume/workspace pei-image:stage-2 /bin/bash Using docker volume as external storage Using docker volumes is preferred if you run the image locally, because it is more efficient, and will not get lost when the container is removed. Docker volumes can be created automatically, or manually with a given name that can be used to mount the volume to the container. This example is based on the basic ssh image , which demonstrates how to use existing docker volumes as external storage. The stage-2 image has three external storage directories: app , data , and workspace (note that the directory names are NOT CUSTOMIZABLE , they are predefined ), where docker volumes are mounted. # user_config.yml stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' apt: repo_source: tuna stage_2: image: output: pei-image:stage-2 storage: app: type: auto-volume data: type: manual-volume volume_name: my_data workspace: type: manual-volume volume_name: my_workspace app is an auto-volume , which means it will be created automatically. data and workspace are manual-volumes , which means they are created manually with the names my_data and my_workspace first, and then mounted to the container. To create these volumes, use the following commands docker volume create my_data docker volume create my_workspace You can check the volumes with docker volume ls . If you are using the docker desktop on Windows, you will see this: In the container, you can access these directories through /soft/app , /soft/data , and /soft/workspace , which are linked to /app , /data , and /workspace under the /hard/volume . With docker run If you are using docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. Note that you will have to create all volumes manully. The command will look like this: # using docker run, you will have to create all volumes manually docker volume create app docker volume create my_data docker volume create my_workspace docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 -v app:/hard/volume/app -v data:/hard/volume/data -v workspace:/hard/volume/workspace pei-image:stage-2 /bin/bash # you don't need to run stage-1 # docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 pei-image:stage-1 /bin/bash Install miniconda in image IMPORTANT NOTE : The following example is for demonstration purposes only. It is not recommended to install miniconda (or any other apps) in the image, because it will make the image size larger, and later modifications (such as conda install xxx ) will get lost when container is removed. It is recommended to install miniconda in the volume storage /hard/volume/app , and copy them to the image storage /hard/image/app when you decide to bake them into image. However, external storage only exists in stage-2 . To install miniconda during build, you can make use of custom scripts. PeiDocker allows you to add your scripts in the project_dir/installation/stage-<1,2>/custom , and then specify them in the user_config.yml file. # user_config.yml stage_1: image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: image data: type: image workspace: type: image custom: on_build: - stage-2/custom/install-my-conda.sh In the above example, the script install-my-conda.sh is placed in the project_dir/installation/stage-2/custom directory. The script will be executed during the build of the stage-2 image. Below is the content of the script. It first checks if the miniconda installation file exists in the /tmp directory, and if not, downloads it from the tuna mirror. Then it installs miniconda to /hard/image/app/miniconda3 , and initializes conda for all users. The conda and pip mirrors are set to the tuna mirror. Important points to note: The script is placed in the project_dir/installation/stage-2/custom directory. The package files are placed in the project_dir/installation/stage-2/tmp directory. You can access the installation directory of the stage-2 image using the INSTALL_DIR_CONTAINER_2 environment variable, likewise for stage-1 . Note it will try to use external storage \\hard\\volume\\app first, but because you have not mounted any external storage, it will use the image storage \\hard\\image\\app . During build, you are root, so you shall execute commands for other users using su - $user -c . Remember to set DEBIAN_FRONTEND=noninteractive to prevent interactive prompts. #!/bin/bash # prevent interactive prompts export DEBIAN_FRONTEND=noninteractive # INSTALL_DIR_CONTAINER_2 points to where the installation/stage-2 is inside container STAGE_2_DIR_IN_CONTAINER=$INSTALL_DIR_CONTAINER_2 echo \"STAGE_2_DIR_IN_CONTAINER: $STAGE_2_DIR_IN_CONTAINER\" # the installation directory of miniconda3 # first check for volume storage at /hard/volume/app, if not found, use /hard/image/app if [ -d \"/hard/volume/app\" ]; then # volume storage takes precedence, note that it only exists in stage-2 CONDA_INSTALL_DIR=\"/hard/volume/app/miniconda3\" else # otherwise, use the image storage CONDA_INSTALL_DIR=\"/hard/image/app/miniconda3\" fi # already installed? skip if [ -d $CONDA_INSTALL_DIR ]; then echo \"miniconda3 is already installed in $CONDA_INSTALL_DIR, skipping ...\" exit 0 fi # download the miniconda3 installation file yourself, and put it in the tmp directory # it will be copied to the container during the build process CONDA_PACKAGE_NAME=\"Miniconda3-latest-Linux-x86_64.sh\" # are you in arm64 platform? If so, use the arm64 version of miniconda3 if [ \"$(uname -m)\" = \"aarch64\" ]; then CONDA_PACKAGE_NAME=\"Miniconda3-latest-Linux-aarch64.sh\" fi # download from CONDA_DOWNLOAD_URL=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/$CONDA_PACKAGE_NAME\" # download to CONDA_DOWNLOAD_DST=\"$STAGE_2_DIR_IN_CONTAINER/tmp/$CONDA_PACKAGE_NAME\" # if the file does not exist, wget it from tuna if [ ! -f $CONDA_DOWNLOAD_DST ]; then echo \"downloading miniconda3 installation file ...\" wget -O $CONDA_DOWNLOAD_DST $CONDA_DOWNLOAD_URL fi # install miniconda3 unattended echo \"installing miniconda3 to $CONDA_INSTALL_DIR ...\" bash $CONDA_DOWNLOAD_DST -b -p $CONDA_INSTALL_DIR # make conda installation read/write for all users echo \"setting permissions for $CONDA_INSTALL_DIR ...\" chmod -R 777 $CONDA_INSTALL_DIR echo \"initializing conda for all users, including root ...\" # conda and pip mirror, for faster python package installation # save the following content to a variable read -r -d '' CONDA_TUNA << EOM channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ EOM # tuna pip mirror read -r -d '' PIP_TUNA << EOM [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple/ [install] trusted-host=pypi.tuna.tsinghua.edu.cn EOM # aliyun pypi mirror, use it if tuna is slow read -r -d '' PIP_ALIYUN << EOM [global] index-url = http://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com EOM # add all user names to USER_LIST USER_LIST=\"root\" for user in $(ls /home); do USER_LIST=\"$USER_LIST $user\" done # for each user in USERS, initialize conda. # remember to execute commands in the user context using su - $user -c # otherwise the file will be owned by root for user in $USER_LIST; do echo \"initializing conda for $user ...\" su - $user -c \"$CONDA_INSTALL_DIR/bin/conda init\" # if user is root, set home_dir to /root, otherwise /home/$user if [ \"$user\" = \"root\" ]; then home_dir=\"/root\" else home_dir=\"/home/$user\" fi # to use tuna mirror, replace the .condarc file with the pre-configured CONDA_TUNA echo \"setting conda mirror for $user ...\" su - $user -c \"echo \\\"$CONDA_TUNA\\\" > $home_dir/.condarc\" # to use pip mirror, create a .pip directory and write the PIP_TUNA to pip.conf echo \"setting pip mirror for $user ...\" su - $user -c \"mkdir -p $home_dir/.pip\" su - $user -c \"echo \\\"$PIP_TUNA\\\" > $home_dir/.pip/pip.conf\" done # create a app-config directory in conda installation directory to save .condarc and .pip directory # because when conda is installed in external storage, these files will be lost after container restart # we can recover them from app-config if needed echo \"creating app-config directory in $CONDA_INSTALL_DIR ...\" mkdir -p $CONDA_INSTALL_DIR/app-config # copy .condarc and .pip directory to app-config echo \"copying .condarc and .pip directory to app-config ...\" cp /root/.condarc $CONDA_INSTALL_DIR/app-config cp -r /root/.pip $CONDA_INSTALL_DIR/app-config # make it accessible to all users echo \"setting permissions for $CONDA_INSTALL_DIR/app-config ...\" chmod -R 777 $CONDA_INSTALL_DIR/app-config Install miniconda to external storage To install miniconda to external storage, you can mount external storage ( How to use external storage? ) to the /hard/volume/app , so that the miniconda installation will be saved there. The following example demonstrates how to install miniconda to the external storage. First, create a docker volume named my_app : docker volume create my_app Then, modify the user_config.yml file as follows: stage_1: image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: manual-volume volume_name: my_app data: type: auto-volume workspace: type: auto-volume custom: on_first_run: - stage-2/custom/install-my-conda.sh The example is based on Install miniconda in image , with the following changes: - app is a manual-volume , which means it is created manually with the name my_app , and then mounted to the container. To create it, use docker volume create my_app . - data and workspace are auto-volumes , which means they are created automatically. - The install-my-conda.sh script is executed on the first run of the container, to install miniconda3 and setup conda for all users. IMPORTANT : Here comes the critical part for using on_first_run commands. After the commands are run, they modify the container , not the image . If you want to save the changes to the image, you need to commit the container to the image. If you forgot to do that, changes are not saved, and the first-run commands will be run again when the container is recreated. To correct this, commit the container to the image after the first run: docker commit <container_id> pei-image:stage-2 Moving external storage to image After you have installed apps to the external storage, you can move it to the image storage. This is useful when you want to bake the installed apps into the image. To do this, just copy the files from the external storage to the image storage. Inside the container, do the followings: # inside container # just copy /hard/volume/app to /hard/image/app, likewise for other directories cp -r /hard/volume/app /hard/image/app # for data and workspace, if you want to bake them into the image # cp -r /hard/volume/data /hard/image/data # cp -r /hard/volume/workspace /hard/image/workspace And then, commit your container to image: docker commit <container_id> pei-image:stage-2 Using proxy You can use proxy when building the image. The following example demonstrates how to use proxy for stage_1 . The proxy is set to host.docker.internal:30080 , which refers to 127.0.0.1:30080 in host where the proxy is running. The apt is specified to use the proxy, and the proxy is kept after the build. stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: false # if set to true, the proxy is used during build and run remove_after_build: false # if set to true, the proxy is removed after build, not effecting run use_https: false # false means http proxy is used where https proxy is requested apt: use_proxy: true # use the proxy for apt keep_proxy_after_build: true # keep apt using the proxy after build Using proxy during build and run This example shows how to use proxy during build and run. The key is to set enable_globally to true . If you set remove_after_build to true , the proxy will be removed after the build, and not effecting the run, this is useful if you only want to use the proxy during build (e.g., pulling sources from github). stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: true remove_after_build: false use_https: false apt: repo_source: tuna Using proxy across stages This example shows how to use proxy across stages. The proxy is set in stage-1 , but only used for apt . In stage-2 , the proxy is enabled globally, affecting all commands after run. stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: false remove_after_build: false use_https: false apt: use_proxy: true keep_proxy_after_build: true stage_2: image: output: pei-image:stage-2 proxy: enable_globally: true You can verify the proxy is set by running env after ssh into the container. # inside container env | grep -i proxy # you should see the proxy settings # http_proxy=http://host.docker.internal:30080 # https_proxy=http://host.docker.internal:30080 Using proxy manually If you specify the proxy, but never use it, it will not automatically affect anything. But you can find the proxy setting in the container, using environment variables PEI_HTTP_PROXY_1 and PEI_HTTPS_PROXY_1 (replace 1 with the stage number). stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 apt: repo_source: tuna After ssh into the container, you can find the proxy settings: # inside container env | grep -i proxy # you should see the proxy settings # PEI_HTTP_PROXY_1=http://host.docker.internal:30080 # PEI_HTTPS_PROXY_1=http://host.docker.internal:30080","title":"Examples"},{"location":"examples/#examples","text":"","title":"Examples"},{"location":"examples/#basic-ubuntu-image-with-ssh-support","text":"This is the most basic example of creating an image with SSH support. The image is based on ubuntu:24.04 and has three users: me , you , and root . The passwords for the users are 123456 , 654321 , and root respectively. The SSH server is running on port 22 and mapped to host port 2222 . To accelerate apt installation in China, the apt source is set to tuna . For other options, see the full config file for documentation. If this option is omitted, the default source will be used. # user_config.yml under project directory stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root apt: repo_source: tuna With this user_config.yml in side your project dir, do the followings to build and run the image: # assuming the project dir is /path/to/project python -m pei_docker.pei configure --project-dir=/path/to/project cd /path/to/project docker compose build stage-1 --progress=plain --no-cache docker compose up stage-1","title":"Basic Ubuntu image with SSH support"},{"location":"examples/#with-docker-run","text":"If you prefer to use docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. The command will look like this: docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 pei-image:stage-1 /bin/bash","title":"With docker run"},{"location":"examples/#gpu-image-with-external-storage","text":"This example is based on the basic ssh image , which demonstrates how to use GPU in the container. The image is based on nvidia/cuda:11.8.0-runtime-ubuntu22.04 that makes use of the GPU. As such, the device section is added to the stage_1 and stage_2 sections, which are set to gpu . stage_1: image: base: nvidia/cuda:11.8.0-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: host host_path: d:/code/PeiDocker/build/storage/app data: type: host host_path: d:/code/PeiDocker/build/storage/data workspace: type: host host_path: d:/code/PeiDocker/build/storage/workspace","title":"GPU image with external storage"},{"location":"examples/#external-storage-with-host-directory","text":"The stage-2 image has three external storage directories: app , data , and workspace (note that the directory names are NOT CUSTOMIZABLE , for arbitrary mounts, see the next section ), where specified host directories are mounted. In the container, you can access these directories through /soft/app , /soft/data , and /soft/workspace , which are linked to /app , /data , and /workspace under the /hard/volume . In this example, the host directories are d:/code/PeiDocker/build/storage/app , d:/code/PeiDocker/build/storage/data , and d:/code/PeiDocker/build/storage/workspace (Windows path).","title":"External storage with host directory"},{"location":"examples/#mounting-arbitrary-volumes-or-directories","text":"You can mount additional volumes or directories to the container by adding them to the mount section. The following example demonstrates how to mount the apt_cache directory to the container's /var/cache/apt directory, in this way, the apt cache will be saved for future use. home_me is mounted to /home/me to save the home directory to a volume, so that it will not get lost after the container is deleted. Unlike app , data , and workspace , the mounted volumes apt_cache and home_me will not be linked to /soft , they are mounted directly to the container and not managed by PeiDocker. You can also use manual-volume (with volume_name set) or host (with host_path set) in ther type though. # heavy duty cpp development, install a lot of things stage_1: # input/output image settings image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 # ssh settings ssh: port: 22 # port in container host_port: 2222 # port in host # ssh users, the key is user name, value is user info users: me: password: '123456' root: # you can configure root user here password: root pubkey_file: null device: type: gpu apt: repo_source: 'tuna' # additional mounts mount: # save apt cache, to speed up future installations apt_cache: type: auto-volume dst_path: /var/cache/apt stage_2: image: output: pei-image:stage-2 device: type: gpu # additional mounts mount: # save home directory to volume, so that it will not get lost after container deletion home_me: type: auto-volume dst_path: /home/me # mount will not be inherited from stage-1 # you need to mount it again, or otherwise it will not be mounted apt_cache: type: auto-volume dst_path: /var/cache/apt","title":"Mounting arbitrary volumes or directories"},{"location":"examples/#with-docker-compose","text":"To build and run the image, do the followings: # assuming the project dir is /path/to/project # build stage-1 docker compose -f /path/to/project/docker-compose.yml build stage-1 --progress=plain --no-cache # build stage-2 docker compose -f /path/to/project/docker-compose.yml build stage-2 --progress=plain --no-cache # start stage-2 docker compose -f /path/to/project/docker-compose.yml up stage-2 After that, you can ssh into the container with the following command: ssh -p 2222 me@127.0.0.1","title":"With docker-compose"},{"location":"examples/#with-docker-run_1","text":"If you are using docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. The command will look like this, note that you need to add --gpus all manually: # you only need to run stage-2 docker run --gpus all -i -t --add-host host.docker.internal:host-gateway -p 2222:22 -v d:/code/PeiDocker/build/storage/app:/hard/volume/app -v d:/code/PeiDocker/build/storage/data:/hard/volume/data -v d:/code/PeiDocker/build/storage/workspace:/hard/volume/workspace pei-image:stage-2 /bin/bash","title":"With docker run"},{"location":"examples/#using-docker-volume-as-external-storage","text":"Using docker volumes is preferred if you run the image locally, because it is more efficient, and will not get lost when the container is removed. Docker volumes can be created automatically, or manually with a given name that can be used to mount the volume to the container. This example is based on the basic ssh image , which demonstrates how to use existing docker volumes as external storage. The stage-2 image has three external storage directories: app , data , and workspace (note that the directory names are NOT CUSTOMIZABLE , they are predefined ), where docker volumes are mounted. # user_config.yml stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' apt: repo_source: tuna stage_2: image: output: pei-image:stage-2 storage: app: type: auto-volume data: type: manual-volume volume_name: my_data workspace: type: manual-volume volume_name: my_workspace app is an auto-volume , which means it will be created automatically. data and workspace are manual-volumes , which means they are created manually with the names my_data and my_workspace first, and then mounted to the container. To create these volumes, use the following commands docker volume create my_data docker volume create my_workspace You can check the volumes with docker volume ls . If you are using the docker desktop on Windows, you will see this: In the container, you can access these directories through /soft/app , /soft/data , and /soft/workspace , which are linked to /app , /data , and /workspace under the /hard/volume .","title":"Using docker volume as external storage"},{"location":"examples/#with-docker-run_2","text":"If you are using docker run to run the image, you can copy-paste the docker-compose.yml file into Decomposerize to get the docker run command. Note that you will have to create all volumes manully. The command will look like this: # using docker run, you will have to create all volumes manually docker volume create app docker volume create my_data docker volume create my_workspace docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 -v app:/hard/volume/app -v data:/hard/volume/data -v workspace:/hard/volume/workspace pei-image:stage-2 /bin/bash # you don't need to run stage-1 # docker run -i -t --add-host host.docker.internal:host-gateway -p 2222:22 pei-image:stage-1 /bin/bash","title":"With docker run"},{"location":"examples/#install-miniconda-in-image","text":"IMPORTANT NOTE : The following example is for demonstration purposes only. It is not recommended to install miniconda (or any other apps) in the image, because it will make the image size larger, and later modifications (such as conda install xxx ) will get lost when container is removed. It is recommended to install miniconda in the volume storage /hard/volume/app , and copy them to the image storage /hard/image/app when you decide to bake them into image. However, external storage only exists in stage-2 . To install miniconda during build, you can make use of custom scripts. PeiDocker allows you to add your scripts in the project_dir/installation/stage-<1,2>/custom , and then specify them in the user_config.yml file. # user_config.yml stage_1: image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: image data: type: image workspace: type: image custom: on_build: - stage-2/custom/install-my-conda.sh In the above example, the script install-my-conda.sh is placed in the project_dir/installation/stage-2/custom directory. The script will be executed during the build of the stage-2 image. Below is the content of the script. It first checks if the miniconda installation file exists in the /tmp directory, and if not, downloads it from the tuna mirror. Then it installs miniconda to /hard/image/app/miniconda3 , and initializes conda for all users. The conda and pip mirrors are set to the tuna mirror. Important points to note: The script is placed in the project_dir/installation/stage-2/custom directory. The package files are placed in the project_dir/installation/stage-2/tmp directory. You can access the installation directory of the stage-2 image using the INSTALL_DIR_CONTAINER_2 environment variable, likewise for stage-1 . Note it will try to use external storage \\hard\\volume\\app first, but because you have not mounted any external storage, it will use the image storage \\hard\\image\\app . During build, you are root, so you shall execute commands for other users using su - $user -c . Remember to set DEBIAN_FRONTEND=noninteractive to prevent interactive prompts. #!/bin/bash # prevent interactive prompts export DEBIAN_FRONTEND=noninteractive # INSTALL_DIR_CONTAINER_2 points to where the installation/stage-2 is inside container STAGE_2_DIR_IN_CONTAINER=$INSTALL_DIR_CONTAINER_2 echo \"STAGE_2_DIR_IN_CONTAINER: $STAGE_2_DIR_IN_CONTAINER\" # the installation directory of miniconda3 # first check for volume storage at /hard/volume/app, if not found, use /hard/image/app if [ -d \"/hard/volume/app\" ]; then # volume storage takes precedence, note that it only exists in stage-2 CONDA_INSTALL_DIR=\"/hard/volume/app/miniconda3\" else # otherwise, use the image storage CONDA_INSTALL_DIR=\"/hard/image/app/miniconda3\" fi # already installed? skip if [ -d $CONDA_INSTALL_DIR ]; then echo \"miniconda3 is already installed in $CONDA_INSTALL_DIR, skipping ...\" exit 0 fi # download the miniconda3 installation file yourself, and put it in the tmp directory # it will be copied to the container during the build process CONDA_PACKAGE_NAME=\"Miniconda3-latest-Linux-x86_64.sh\" # are you in arm64 platform? If so, use the arm64 version of miniconda3 if [ \"$(uname -m)\" = \"aarch64\" ]; then CONDA_PACKAGE_NAME=\"Miniconda3-latest-Linux-aarch64.sh\" fi # download from CONDA_DOWNLOAD_URL=\"https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/$CONDA_PACKAGE_NAME\" # download to CONDA_DOWNLOAD_DST=\"$STAGE_2_DIR_IN_CONTAINER/tmp/$CONDA_PACKAGE_NAME\" # if the file does not exist, wget it from tuna if [ ! -f $CONDA_DOWNLOAD_DST ]; then echo \"downloading miniconda3 installation file ...\" wget -O $CONDA_DOWNLOAD_DST $CONDA_DOWNLOAD_URL fi # install miniconda3 unattended echo \"installing miniconda3 to $CONDA_INSTALL_DIR ...\" bash $CONDA_DOWNLOAD_DST -b -p $CONDA_INSTALL_DIR # make conda installation read/write for all users echo \"setting permissions for $CONDA_INSTALL_DIR ...\" chmod -R 777 $CONDA_INSTALL_DIR echo \"initializing conda for all users, including root ...\" # conda and pip mirror, for faster python package installation # save the following content to a variable read -r -d '' CONDA_TUNA << EOM channels: - defaults show_channel_urls: true default_channels: - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/main - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/r - https://mirrors.tuna.tsinghua.edu.cn/anaconda/pkgs/msys2 custom_channels: conda-forge: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud msys2: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud bioconda: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud menpo: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud pytorch-lts: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud simpleitk: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud deepmodeling: https://mirrors.tuna.tsinghua.edu.cn/anaconda/cloud/ EOM # tuna pip mirror read -r -d '' PIP_TUNA << EOM [global] index-url = https://pypi.tuna.tsinghua.edu.cn/simple/ [install] trusted-host=pypi.tuna.tsinghua.edu.cn EOM # aliyun pypi mirror, use it if tuna is slow read -r -d '' PIP_ALIYUN << EOM [global] index-url = http://mirrors.aliyun.com/pypi/simple/ [install] trusted-host=mirrors.aliyun.com EOM # add all user names to USER_LIST USER_LIST=\"root\" for user in $(ls /home); do USER_LIST=\"$USER_LIST $user\" done # for each user in USERS, initialize conda. # remember to execute commands in the user context using su - $user -c # otherwise the file will be owned by root for user in $USER_LIST; do echo \"initializing conda for $user ...\" su - $user -c \"$CONDA_INSTALL_DIR/bin/conda init\" # if user is root, set home_dir to /root, otherwise /home/$user if [ \"$user\" = \"root\" ]; then home_dir=\"/root\" else home_dir=\"/home/$user\" fi # to use tuna mirror, replace the .condarc file with the pre-configured CONDA_TUNA echo \"setting conda mirror for $user ...\" su - $user -c \"echo \\\"$CONDA_TUNA\\\" > $home_dir/.condarc\" # to use pip mirror, create a .pip directory and write the PIP_TUNA to pip.conf echo \"setting pip mirror for $user ...\" su - $user -c \"mkdir -p $home_dir/.pip\" su - $user -c \"echo \\\"$PIP_TUNA\\\" > $home_dir/.pip/pip.conf\" done # create a app-config directory in conda installation directory to save .condarc and .pip directory # because when conda is installed in external storage, these files will be lost after container restart # we can recover them from app-config if needed echo \"creating app-config directory in $CONDA_INSTALL_DIR ...\" mkdir -p $CONDA_INSTALL_DIR/app-config # copy .condarc and .pip directory to app-config echo \"copying .condarc and .pip directory to app-config ...\" cp /root/.condarc $CONDA_INSTALL_DIR/app-config cp -r /root/.pip $CONDA_INSTALL_DIR/app-config # make it accessible to all users echo \"setting permissions for $CONDA_INSTALL_DIR/app-config ...\" chmod -R 777 $CONDA_INSTALL_DIR/app-config","title":"Install miniconda in image"},{"location":"examples/#install-miniconda-to-external-storage","text":"To install miniconda to external storage, you can mount external storage ( How to use external storage? ) to the /hard/volume/app , so that the miniconda installation will be saved there. The following example demonstrates how to install miniconda to the external storage. First, create a docker volume named my_app : docker volume create my_app Then, modify the user_config.yml file as follows: stage_1: image: base: nvidia/cuda:12.3.2-runtime-ubuntu22.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' root: password: root apt: repo_source: tuna device: type: gpu stage_2: image: output: pei-image:stage-2 device: type: gpu storage: app: type: manual-volume volume_name: my_app data: type: auto-volume workspace: type: auto-volume custom: on_first_run: - stage-2/custom/install-my-conda.sh The example is based on Install miniconda in image , with the following changes: - app is a manual-volume , which means it is created manually with the name my_app , and then mounted to the container. To create it, use docker volume create my_app . - data and workspace are auto-volumes , which means they are created automatically. - The install-my-conda.sh script is executed on the first run of the container, to install miniconda3 and setup conda for all users. IMPORTANT : Here comes the critical part for using on_first_run commands. After the commands are run, they modify the container , not the image . If you want to save the changes to the image, you need to commit the container to the image. If you forgot to do that, changes are not saved, and the first-run commands will be run again when the container is recreated. To correct this, commit the container to the image after the first run: docker commit <container_id> pei-image:stage-2","title":"Install miniconda to external storage"},{"location":"examples/#moving-external-storage-to-image","text":"After you have installed apps to the external storage, you can move it to the image storage. This is useful when you want to bake the installed apps into the image. To do this, just copy the files from the external storage to the image storage. Inside the container, do the followings: # inside container # just copy /hard/volume/app to /hard/image/app, likewise for other directories cp -r /hard/volume/app /hard/image/app # for data and workspace, if you want to bake them into the image # cp -r /hard/volume/data /hard/image/data # cp -r /hard/volume/workspace /hard/image/workspace And then, commit your container to image: docker commit <container_id> pei-image:stage-2","title":"Moving external storage to image"},{"location":"examples/#using-proxy","text":"You can use proxy when building the image. The following example demonstrates how to use proxy for stage_1 . The proxy is set to host.docker.internal:30080 , which refers to 127.0.0.1:30080 in host where the proxy is running. The apt is specified to use the proxy, and the proxy is kept after the build. stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: false # if set to true, the proxy is used during build and run remove_after_build: false # if set to true, the proxy is removed after build, not effecting run use_https: false # false means http proxy is used where https proxy is requested apt: use_proxy: true # use the proxy for apt keep_proxy_after_build: true # keep apt using the proxy after build","title":"Using proxy"},{"location":"examples/#using-proxy-during-build-and-run","text":"This example shows how to use proxy during build and run. The key is to set enable_globally to true . If you set remove_after_build to true , the proxy will be removed after the build, and not effecting the run, this is useful if you only want to use the proxy during build (e.g., pulling sources from github). stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: true remove_after_build: false use_https: false apt: repo_source: tuna","title":"Using proxy during build and run"},{"location":"examples/#using-proxy-across-stages","text":"This example shows how to use proxy across stages. The proxy is set in stage-1 , but only used for apt . In stage-2 , the proxy is enabled globally, affecting all commands after run. stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 enable_globally: false remove_after_build: false use_https: false apt: use_proxy: true keep_proxy_after_build: true stage_2: image: output: pei-image:stage-2 proxy: enable_globally: true You can verify the proxy is set by running env after ssh into the container. # inside container env | grep -i proxy # you should see the proxy settings # http_proxy=http://host.docker.internal:30080 # https_proxy=http://host.docker.internal:30080","title":"Using proxy across stages"},{"location":"examples/#using-proxy-manually","text":"If you specify the proxy, but never use it, it will not automatically affect anything. But you can find the proxy setting in the container, using environment variables PEI_HTTP_PROXY_1 and PEI_HTTPS_PROXY_1 (replace 1 with the stage number). stage_1: image: base: ubuntu:24.04 output: pei-image:stage-1 ssh: enable: true port: 22 host_port: 2222 users: me: password: '123456' you: password: '654321' root: password: root proxy: address: host.docker.internal port: 30080 apt: repo_source: tuna After ssh into the container, you can find the proxy settings: # inside container env | grep -i proxy # you should see the proxy settings # PEI_HTTP_PROXY_1=http://host.docker.internal:30080 # PEI_HTTPS_PROXY_1=http://host.docker.internal:30080","title":"Using proxy manually"}]}